
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Step 7: Property Proving</title><meta name="generator" content="MATLAB 8.6"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2015-11-05"><meta name="DC.source" content="Step_07.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:14px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:2.0em; color:#000077; line-height:150%; font-weight:bold; text-align:center }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.6em; color:#444444; font-weight:bold; font-style:italic; text-align:left; vertical-align:bottom; line-height:200%; border-top:2px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#555555; font-style:italic; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px;} 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Step 7: Property Proving</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Introduction</a></li><li><a href="#2">Verification and Validation Tools Used</a></li><li><a href="#3">Reproducing the Field Issue</a></li><li><a href="#4">Proving the Model will Meet Requirements</a></li><li><a href="#5">Summary</a></li></ul></div><h2>Introduction<a name="1"></a></h2><p><img vspace="5" hspace="5" src="Step_07_CruiseControl_WhatNow.png" alt=""> </p><p>Up until now, our discussion on Verification and Validation has been centered on testing using a set of test vectors as the input. Test vectors can be generated by an engineer working from a requirements document or automatically generated using Simulink Design Verifier. The goal of course is to achieve 100% MCDC coverage using requirement- or function-based test vectors.  As one can imagine, this method can be time-consuming, but there is always room for potential errors to occur.</p><p>When it comes to requirements that are critical for safety reasons or have significant importance, it is always good to have different methods for looking at the same problem, or redundancy in the verification. This will add quality to the final product.</p><p>In general, the quality of the verification process is a function of your input vectors. In an ideal world, every combination of states, timing sequence, etc. needs to be checked. However, this is often not practical due to the time and effort that would be required. This gap often results in model error caused by race or initialization conditions. With that in mind, we are going to introduce a different method of verification called <b>Property Proving</b>. The idea behind this method is that instead of engineers generating numerous test vectors, engineers would "model" the requirement or property and ask the tool to "prove" that the implementation or design meets the requirement or property.</p><p>The tool we are going to use is <b>Simulink Design Verifier</b>, the same tool we used for design error detection and automatic test generation. <b>Simulink Design Verifier</b> extends Simulink by using formal methods to deterministically prove or disprove your implementation model. We need to acknowledge that the underlying technology of the tool is from Prover&reg; Technology, a technology that is proven in other software environments.</p><h2>Verification and Validation Tools Used<a name="2"></a></h2><div><ul><li>Simulink Design Verifier</li></ul></div><h2>Reproducing the Field Issue<a name="3"></a></h2><p>With the previous steps in the workshop we found design issues and fixed them early.  With the increased confidence from the earlier work we thought we were ready for field testing.  We were suprised when the field calibrator reported this issue:</p><p><img vspace="5" hspace="5" src="Step_07_FieldIssue.png" alt=""> </p><p><b>Problem: While going downhill, target speed increases with &#8220;reduce speed&#8221; button and assumes random values</b></p><p>How can we debug the issues found in the field?</p><div><ul><li>Reproduce field issue in Simulink:  Use field calibration values and set inputs manually or replay data if it exists</li><li>But this is just &#8220;one&#8221; more test case that covers a subset of all possible input ranges, sequences and calibration values</li><li>We already have requirements based test vectors, 100% coverage, and all passed</li><li>How do I know that this will not happen with a different set of conditions?  Road conditions, driver inputs, calibration values and/or sequence of events</li></ul></div><p>Let's use <b>Simulink Design Verifier</b> to help reproduce this test conditon.  To do this we will model the field issue in Simulink:</p><div><ul><li>Create model of field issue behavior</li><li>Constrain the inputs to represent the field issue</li><li>Ask <b>Simulink Design Verifier</b> to prove whether errant condition can occur</li></ul></div><p>To see how we can model this field issue, do the following:</p><p>1.  Open <b>CruiseControl_pp.slx</b> with the field issue model &#8211; <b><a href="matlab:D1_PP_FieldIssue;">click here</a></b>.</p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssue.png" alt=""> </p><p>2.  Examine the contents of the property proving model.</p><p>The left subsystem block, <b>Input_Constraints</b> contains the input constraints to match the field issue including:</p><div><ul><li>Cruise power was always on</li><li>Brake was always off</li><li>Increase speed button (AccelResSW) was always off</li></ul></div><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueInput.png" alt=""> </p><p>The center subsystem block, <b>Controls</b> contains our cruise control algorithm we tested before going to the vehicle.</p><p>The right subsystem block, <b>Field_Issue_Behavior</b> contains a model of the behavior as described in the problem report e-mail above:</p><div><ul><li>Target speed was already set once, so it was an engaged "during" event</li><li>Pulsing the "reduce speed" button, so it was off the time step before</li><li>The target speed increased with the "reduce speed" button.  This should never happen so this is set to "false"  with the <b>NOT</b> block and we will ask the tool to prove that this can never happen.  If it can't prove this true then it will create a test to show how it is false.</li></ul></div><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueBehavMdl.png" alt=""> </p><p><b>Simulink Design Verifier</b> provides a library that we used to in the <b>Input_Constraints</b> subsystem and the <b>Field_Issue_Behavior</b> subsystem to identify how signals are constrained (<b>Assumptions</b>) and what signals are proofs (<b>Proof Objectives</b>).</p><p><img vspace="5" hspace="5" src="Step_07_SLDVlib.png" alt=""> </p><p>3.  Run the analysis, select <b>Analysis</b>, <b>Design Verifier</b>, <b>Prove Properties</b> and then <b>Model</b>.</p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueRunProof.png" alt=""> </p><p>The progress window list the current tasks of the analysis including compiling the model and translating the model to what is required by the formal analysis engine of <b>Simulink Design Verifier</b>.</p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueProgress.png" alt=""> </p><p>The <b>Simulink Design Verifier Results Inspector</b> window appears to provide a summary of the results including hyperlinks to the harness model and the analysis report.  The results shown in the window indicates that <b>Simulink Design Verifier</b> has falsified the proof, which is also displayed on the <b>Field_Issue_Behavior</b> subsystem as a red outline indicating the proof objective inside has been falsified.</p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueResults.png" alt=""> </p><p>4.  To open the generated harness, and begin debugging the issue, double-click the <b>Open Harness</b> block.  This will bring up a preconfigured harness window that will assist the debugging effort.</p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueTestHarness.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueSigBldr.png" alt=""> </p><p>5.  Run the test case and open <b>Simulation Data Inspector (SDI)</b>.</p><p>6.  Configure <b>SDI</b> to be a <b>3x1</b> format</p><p><img vspace="5" hspace="5" src="Step_07_PP_SDIformat.png" alt=""> </p><p>7.  Assign <b>tspeed</b> to the top plot, <b>CoastSetSw</b> to the middle plot and <b>AccelResSw</b> to the bottom plot</p><p>8.  Press the icon on the toolstrip shown below to fit the data</p><p><img vspace="5" hspace="5" src="Step_07_PP_SDIfitData.png" alt=""> </p><p>9.  Select <b>Overwrite Run</b> so each subsequent run will overwrite the previous run and be displayed on the plots.</p><p><img vspace="5" hspace="5" src="Step_07_PP_SDIoverwriteRun.png" alt=""> </p><p>10. Select <b>Data Cursors</b> then <b>Two</b> cursors.  Position the cursors at (0.3) and (0.4) seconds.</p><p><b>SDI</b> window should now look like this:</p><p><img vspace="5" hspace="5" src="Step_07_PP_SDIconfigured.png" alt=""> </p><p>Notice the <b>target speed</b> has increase from (20) to (25) when the <b>CoastSetSw</b> (reduce speed button) was pushed.  We will re-run the simulation but this time we will look at the logic to debug the issue.</p><p>11. Open the <b>Compute target speed</b> state chart</p><p>12. Restart the simulation, but this time use the single-stepping buttons.</p><p><img vspace="5" hspace="5" src="Step_07_PP_SingleStep.png" alt=""> </p><p>13. Repeat single-stepping until the simulation time is (0.4), observing the behavior in the state chart.  At (0.4) notice the path as shown below.</p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueSF.png" alt=""> </p><p>The bug is a "sneak path" that was not detected with our simulation tests but was found in the field by the calibrator and reproduce in the model with <b>Property Proving</b>.</p><p>14. Select the fix by double-clicking the <b>ComponentVersion</b> and selecting <b>Execution Order Sneak Fix</b> from the dropdown.  Make sure you select this in the <b>CruiseControl_pp_harness_FieldIssue</b> model and not <b>CruiseControl_pp</b>.</p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueSelectFix.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_07_PPfieldIssueFix.png" alt=""> </p><p>15. Rerun the simulation with the fix and view the results in <b>SDI</b>.</p><p><img vspace="5" hspace="5" src="Step_07_PP_FieldIssueSDIfix.png" alt=""> </p><h2>Proving the Model will Meet Requirements<a name="4"></a></h2><p>In the previous section we used <b>Simulink Design Verifier</b> to reproduce a field issue.  This is using the tool in a "reactive" mode, what about using the tool in a "proactive" way?  We can certainly model a requirement in a more general way than a field issue and ask the tool to falsify or show a way for the implementation to not meet requirements. So underlying the field issue is a requirement:</p><p><img vspace="5" hspace="5" src="Step_07_ReqToModel.png" alt=""> </p><p>To see how we can model this field issue, do the following:</p><p>1.  Open <b>CruiseControl_pp.slx</b> with the requirement behavior model &#8211; <b><a href="matlab:D2_PP_ReqProve;">click here</a></b>.</p><p><img vspace="5" hspace="5" src="Step_07_PPreqProve.png" alt=""> </p><p>2.  Examine the contents of the property proving model.</p><p>The left subsystem block, <b>Input_Constraints</b> contains the input constraints to match the requirement.  So in this case, there are no constraints specified by the requirement.</p><p><img vspace="5" hspace="5" src="Step_07_PPreqProveInput.png" alt=""> </p><p>The center subsystem block, <b>Controls</b> again contains our cruise control algorithm we tested before going to the vehicle plus the "sneak" fix that we created in the previous section.</p><p>The right subsystem block, <b>Field_Issue_Behavior</b> contains a model of the behavior as described in the problem report e-mail above:</p><div><ul><li>Applies only to the engaged "during" condition</li><li>No constraint on whether the "reduce speed" is being pulsed, held or any other sequence</li><li>The target speed should not increase with the "reduce speed" button</li><li>AND the target speed should never fall below the limit</li></ul></div><p><img vspace="5" hspace="5" src="Step_07_PPreqProveBehavMdl.png" alt=""> </p><p>3.  Run the analysis, select <b>Analysis</b>, <b>Design Verifier</b>, <b>Prove Properties</b> and then <b>Model</b>.</p><p>The <b>Simulink Design Verifier Results Inspector</b> window appears as before to provide a summary of the results.  The result shown in the window indicates that <b>Simulink Design Verifier</b> has falsified the "requirement" proof, which is also shown on the <b>Field_Issue_Behavior</b> subsystem as a red outline indicating the proof objective inside has been falsified.</p><p><img vspace="5" hspace="5" src="Step_07_PPreqProveResults.png" alt=""> </p><p>4.  To open the generated harness, and begin debugging the issue, double-click the <b>Open Harness</b> block.  This will bring up a preconfigured harness window that will assist the debugging effort.</p><p><img vspace="5" hspace="5" src="Step_07_PPreqProveTestHarness.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_07_PPreqProveSigBldr.png" alt=""> </p><p>5.  Run the test case</p><p>6.  Open <b>Simulation Data Inspector (SDI)</b>.  It should still be configured properly from the previous section.</p><p>7.  Position the cursors at (0.2) and (0.3) seconds.</p><p><b>SDI</b> window should now look like this:</p><p><img vspace="5" hspace="5" src="Step_07_PP_reqProveIssueSDI.png" alt=""> </p><p>Based on the results in <b>SDI</b>, <b>Simulink Design Verifier</b> falsified our requirement by creating a test case with simultaneous "push" of both the <b>CoastSetSw</b> (reduce speed button) and the <b>AccelResSw</b> (increase speed button).</p><p>8.  Single-step the simulation to see the behavior in the <b>Compute target speed</b> state chart.  You may want to "Enable stepping back" to replay the previous time steps.</p><p><img vspace="5" hspace="5" src="Step_07_PP_EnableSteppingBack.png" alt=""> </p><p>In the <b>CruiseControl</b> logic you can see that it takes the "increase speed" path due to the execution order in the state chart.</p><p><img vspace="5" hspace="5" src="Step_07_PP_reqProveIssueSF.png" alt=""> </p><p>The order was not specified in the requirement so it is not behavior that is owned by a requirement, and it is not really intended behavior.  You could argue that this behavior is random, and had a 50% chance of being part of the design.  One way to handle this is to add a requirement that would reject a double press event, and then add the implemenation as shown below.</p><p><img vspace="5" hspace="5" src="Step_07_PP_reqProveFixTop.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_07_PP_reqProveFixSub.png" alt=""> </p><p>9. To use this fix, <b>Execution Order Sneak Double Reject Fix</b> in the harness, double-click the <b>Component Version</b> block and select this version.</p><p><img vspace="5" hspace="5" src="Step_07_PP_reqProveFixSelect.png" alt=""> </p><p>10. Rerun the simulation with the fix and view the results in <b>SDI</b>.</p><p><img vspace="5" hspace="5" src="Step_07_PP_reqProveFixSDI.png" alt=""> </p><p>11. Close the harness model and go back to the original model <b>CruiseControl_pp</b>.  We need to update the <b>Controls</b> block with the fix. Double-click the <b>Component Version</b> block and select <b>Execution Order Sneak Double Reject Fix</b> version.</p><p>12. Save the model.</p><p>13. Now re-run <b>Property Proving</b> on the model. Select <b>Analysis</b>, <b>Design Verifier</b>, <b>Prove Properties</b> and then <b>Model</b>.</p><p>The <b>Simulink Design Verifier Results Inspector</b> window appears as before to provide a summary of the results.  The result shown in the window indicates that <b>Simulink Design Verifier</b> has proven that we will always meet the requirement as modeled in the <b>Field_Issue_Behavior</b> subsystem, and now the subsystem as a <b>green</b> outline indicating the proof objective was proven valid.</p><p><img vspace="5" hspace="5" src="Step_07_PPreqProveResultsPass.png" alt=""> </p><h2>Summary<a name="5"></a></h2><p>During the <b>Property Proving</b> we have demonstrated that <b>Simulink Design Verifier</b> can be used to:</p><div><ul><li>"Reactively" reproduce, debug and fix field issues</li><li>"Proactively" prove the implementation will meet requirements</li></ul></div><p>In both use cases, a test case and harness will be generated to help isolate and fix the issue the analysis uncovered.  Once the issue has been resolved in the harness then we need to go back to the <b>Property Proving</b> model to ask if <b>Simulink Design Verifier</b> can find other test cases to falsify our proofs.  The key takeaway is that you save the property models to rerun <b>Property Proving</b> on each new version of the implementation.</p><p>In the workshop we used <b>Property Proving</b> in a "reactive" manner to show it can help with hard to reproduce field issues.  But going forward we would make <b>Property Proving</b> as part of "proactive" workflow where we would select several key or safety critical requirements to run on our implementation before our code generation/verification step.</p><p>It is interesting to note that we diligently ran our requirements based simulation tests, passed the functional tests with 100% coverage and still there may be issues.  Formal methods test more of the design or operating space than simulation based test methods as shown below.</p><p><img vspace="5" hspace="5" src="Step_07_PP_DesignSpace.png" alt=""> </p><p>The <b>Property Proving</b> step of the process is about having confidence in our design meeting requirements across a greater range of operating conditions.  We have now answered all the questions as we have now demonstrated a structured and formal testing framework for securing the quality, robustness and safety of our cruise controller.</p><p><img vspace="5" hspace="5" src="Step_07_CruiseControl_Summary.png" alt=""> </p><p><b><i>CONGRATULATIONS! YOU HAVE NOW COMPLETED ALL THE WORKSHOP EXERCISES!</i></b></p><div><ol><li>When you are finished, close all models and files - or <b><a href="matlab:bdclose('all');">click here</a></b>.</li><li>Go to <b>Step 8: Workflow Summary</b> - <b><a href="Step_08.html">click here</a></b>.</li></ol></div><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2015b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Step 7: Property Proving
%
%% Introduction
%
% <<Step_07_CruiseControl_WhatNow.png>>
%
% Up until now, our discussion on Verification and Validation has been
% centered on testing using a set of test vectors as the input. Test
% vectors can be generated by an engineer working from a requirements
% document or automatically generated using Simulink Design Verifier. The
% goal of course is to achieve 100% MCDC coverage using requirement- or
% function-based test vectors.  As one can imagine, this method can be
% time-consuming, but there is always room for potential errors to occur.
%
% When it comes to requirements that are critical for safety reasons or
% have significant importance, it is always good to have different methods
% for looking at the same problem, or redundancy in the verification. This
% will add quality to the final product.
%
% In general, the quality of the verification process is a function of your
% input vectors. In an ideal world, every combination of states, timing
% sequence, etc. needs to be checked. However, this is often not practical
% due to the time and effort that would be required. This gap often
% results in model error caused by race or initialization conditions.    
% With that in mind, we are going to introduce a different method of
% verification called *Property Proving*. The idea behind this method
% is that instead of engineers generating numerous test vectors, engineers
% would "model" the requirement or property and ask the tool to "prove"
% that the implementation or design meets the requirement or property.
%
% The tool we are going to use is *Simulink Design Verifier*, the same tool
% we used for design error detection and automatic test generation. 
% *Simulink Design Verifier* extends Simulink by using formal methods to 
% deterministically prove or disprove your implementation model. We need to
% acknowledge that the underlying technology of the tool is from Prover® 
% Technology, a technology that is proven in other software environments.
%  
%% Verification and Validation Tools Used
% 
% * Simulink Design Verifier
%
%% Reproducing the Field Issue
%
% With the previous steps in the workshop we found design issues and fixed
% them early.  With the increased confidence from the earlier work we
% thought we were ready for field testing.  We were suprised when the field
% calibrator reported this issue:
% 
% <<Step_07_FieldIssue.png>>
%
% *Problem: While going downhill, target speed increases with “reduce speed” 
% button and assumes random values*
%
% How can we debug the issues found in the field? 
%
% * Reproduce field issue in Simulink:  Use field calibration values and
% set inputs manually or replay data if it exists 
% * But this is just “one” more test case that covers a subset of all 
% possible input ranges, sequences and calibration values
% * We already have requirements based test vectors, 100% coverage, and all passed
% * How do I know that this will not happen with a different set of
% conditions?  Road conditions, driver inputs, calibration values and/or
% sequence of events
%
% Let's use *Simulink Design Verifier* to help reproduce this test
% conditon.  To do this we will model the field issue in Simulink:
%
% * Create model of field issue behavior
% * Constrain the inputs to represent the field issue
% * Ask *Simulink Design Verifier* to prove whether errant condition can occur
%
% To see how we can model this field issue, do the following:
%
% 1.  Open *CruiseControl_pp.slx* with the field issue model – *<matlab:D1_PP_FieldIssue;
% click here>*.
%
% <<Step_07_PPfieldIssue.png>>
%
% 2.  Examine the contents of the property proving model.
%
% The left subsystem block, *Input_Constraints* contains the input 
% constraints to match the field issue
% including:
%
% * Cruise power was always on
% * Brake was always off
% * Increase speed button (AccelResSW) was always off
%
% <<Step_07_PPfieldIssueInput.png>>
%
% The center subsystem block, *Controls* contains our cruise control
% algorithm we tested before going to the vehicle.
%
% The right subsystem block, *Field_Issue_Behavior* contains a model of the
% behavior as described in the problem report e-mail above:
%
% * Target speed was already set once, so it was an engaged "during" event
% * Pulsing the "reduce speed" button, so it was off the time step before
% * The target speed increased with the "reduce speed" button.  This should
% never happen so this is set to "false"  with the *NOT* block and we will
% ask the tool to prove that this can never happen.  If it can't prove this
% true then it will create a test to show how it is false.
%
% <<Step_07_PPfieldIssueBehavMdl.png>>
%
% *Simulink Design Verifier* provides a library that we used to in the
% *Input_Constraints* subsystem and the *Field_Issue_Behavior* subsystem to
% identify how signals are constrained (*Assumptions*) and what signals are 
% proofs (*Proof Objectives*).
%
% <<Step_07_SLDVlib.png>>
%
% 3.  Run the analysis, select *Analysis*, *Design Verifier*, *Prove
% Properties* and then *Model*.
%
% <<Step_07_PPfieldIssueRunProof.png>>
% 
% The progress window list the current tasks of the analysis including
% compiling the model and translating the model to what is required by the
% formal analysis engine of *Simulink Design Verifier*.
%
% <<Step_07_PPfieldIssueProgress.png>>
% 
% The *Simulink Design Verifier Results Inspector* window appears to 
% provide a summary of the results including hyperlinks to the harness
% model and the analysis report.  The results shown in the window indicates
% that *Simulink Design Verifier* has falsified the proof, which is also
% displayed on the *Field_Issue_Behavior* subsystem as a red outline 
% indicating the proof objective inside has been falsified.
%
% <<Step_07_PPfieldIssueResults.png>>
%
% 4.  To open the generated harness, and begin debugging the issue,
% double-click the *Open Harness* block.  This will bring up a
% preconfigured harness window that will assist the debugging effort.
%
% <<Step_07_PPfieldIssueTestHarness.png>>
%
% <<Step_07_PPfieldIssueSigBldr.png>>
%
% 5.  Run the test case and open *Simulation Data Inspector (SDI)*.  
% 
% 6.  Configure *SDI* to be a *3x1* format
%
% <<Step_07_PP_SDIformat.png>>
%
% 7.  Assign *tspeed* to the top plot, *CoastSetSw* to the middle plot and
% *AccelResSw* to the bottom plot
%
% 8.  Press the icon on the toolstrip shown below to fit the data 
%
% <<Step_07_PP_SDIfitData.png>>
%
% 9.  Select *Overwrite Run* so each subsequent run will overwrite the
% previous run and be displayed on the plots.
%
% <<Step_07_PP_SDIoverwriteRun.png>>
%
% 10. Select *Data Cursors* then *Two* cursors.  Position the cursors at
% (0.3) and (0.4) seconds.
%
% *SDI* window should now look like this:
%
% <<Step_07_PP_SDIconfigured.png>>
%
% Notice the *target speed* has increase from (20) to (25) when the
% *CoastSetSw* (reduce speed button) was pushed.  We will re-run the
% simulation but this time we will look at the logic to debug the issue.
%
% 11. Open the *Compute target speed* state chart
%
% 12. Restart the simulation, but this time use the single-stepping buttons.
% 
% <<Step_07_PP_SingleStep.png>>
%
% 13. Repeat single-stepping until the simulation time is (0.4), observing 
% the behavior in the state chart.  At (0.4) notice the path as shown below.
%
% <<Step_07_PPfieldIssueSF.png>>
%
% The bug is a "sneak path" that was not detected with our simulation
% tests but was found in the field by the calibrator and reproduce in the
% model with *Property Proving*.
%
% 14. Select the fix by double-clicking the *ComponentVersion* and selecting 
% *Execution Order Sneak Fix* from the dropdown.  Make sure you select this
% in the *CruiseControl_pp_harness_FieldIssue* model and not
% *CruiseControl_pp*.
%
% <<Step_07_PPfieldIssueSelectFix.png>>
%
% <<Step_07_PPfieldIssueFix.png>>
%
% 15. Rerun the simulation with the fix and view the results in *SDI*.
%
% <<Step_07_PP_FieldIssueSDIfix.png>>
%
%% Proving the Model will Meet Requirements
%
% In the previous section we used *Simulink Design Verifier* to reproduce a
% field issue.  This is using the tool in a "reactive" mode, what about 
% using the tool in a "proactive" way?  We can certainly model a 
% requirement in a more general way than a field issue and ask the tool to 
% falsify or show a way for the implementation to not meet requirements. So
% underlying the field issue is a requirement:
%
% <<Step_07_ReqToModel.png>>
%
% To see how we can model this field issue, do the following:
%
% 1.  Open *CruiseControl_pp.slx* with the requirement behavior model – *<matlab:D2_PP_ReqProve;
% click here>*.
%
% <<Step_07_PPreqProve.png>>
%
% 2.  Examine the contents of the property proving model.
%
% The left subsystem block, *Input_Constraints* contains the input 
% constraints to match the requirement.  So in this case, there are no
% constraints specified by the requirement.
%
% <<Step_07_PPreqProveInput.png>>
%
% The center subsystem block, *Controls* again contains our cruise control
% algorithm we tested before going to the vehicle plus the "sneak" fix that
% we created in the previous section. 
%
% The right subsystem block, *Field_Issue_Behavior* contains a model of the
% behavior as described in the problem report e-mail above:
%
% * Applies only to the engaged "during" condition
% * No constraint on whether the "reduce speed" is being pulsed, held or 
% any other sequence
% * The target speed should not increase with the "reduce speed" button
% * AND the target speed should never fall below the limit
%
% <<Step_07_PPreqProveBehavMdl.png>>
%
% 3.  Run the analysis, select *Analysis*, *Design Verifier*, *Prove
% Properties* and then *Model*.
%
% The *Simulink Design Verifier Results Inspector* window appears as before
% to provide a summary of the results.  The result shown in the window indicates
% that *Simulink Design Verifier* has falsified the "requirement" proof, 
% which is also shown on the *Field_Issue_Behavior* subsystem as a red 
% outline indicating the proof objective inside has been falsified.
%
% <<Step_07_PPreqProveResults.png>>
%
% 4.  To open the generated harness, and begin debugging the issue,
% double-click the *Open Harness* block.  This will bring up a
% preconfigured harness window that will assist the debugging effort.
%
% <<Step_07_PPreqProveTestHarness.png>>
%
% <<Step_07_PPreqProveSigBldr.png>>
%
% 5.  Run the test case
% 
% 6.  Open *Simulation Data Inspector (SDI)*.  It should still be
% configured properly from the previous section.
%
% 7.  Position the cursors at (0.2) and (0.3) seconds.
%
% *SDI* window should now look like this:
%
% <<Step_07_PP_reqProveIssueSDI.png>> 
%
% Based on the results in *SDI*, *Simulink Design Verifier* falsified our
% requirement by creating a test case with simultaneous "push" of both the
% *CoastSetSw* (reduce speed button) and the *AccelResSw* (increase speed
% button).  
%
% 8.  Single-step the simulation to see the behavior in the *Compute target
% speed* state chart.  You may want to "Enable stepping back" to replay the 
% previous time steps.
%
% <<Step_07_PP_EnableSteppingBack.png>>
%
% In the *CruiseControl* logic you can see that it takes the "increase
% speed" path due to the execution order in the state chart.  
%
% <<Step_07_PP_reqProveIssueSF.png>> 
%
% The order was not specified in the requirement so it is not behavior that
% is owned by a requirement, and it is not really intended behavior.  You 
% could argue that this behavior is random, and had a 50% chance of being part 
% of the design.  One way to handle this is to add a requirement that would 
% reject a double press event, and then add the implemenation as shown below.
%
% <<Step_07_PP_reqProveFixTop.png>> 
%
% <<Step_07_PP_reqProveFixSub.png>> 
%
% 9. To use this fix, *Execution Order Sneak Double Reject Fix* in the 
% harness, double-click the *Component Version* block and select this 
% version.
%
% <<Step_07_PP_reqProveFixSelect.png>>
%
% 10. Rerun the simulation with the fix and view the results in *SDI*.
%
% <<Step_07_PP_reqProveFixSDI.png>>
%
% 11. Close the harness model and go back to the original model 
% *CruiseControl_pp*.  We need to update the *Controls* block with the fix. 
% Double-click the *Component Version* block and select 
% *Execution Order Sneak Double Reject Fix* version.
%
% 12. Save the model.
%
% 13. Now re-run *Property Proving* on the model. Select *Analysis*, 
% *Design Verifier*, *Prove Properties* and then *Model*.  
%
% The *Simulink Design Verifier Results Inspector* window appears as before
% to provide a summary of the results.  The result shown in the window indicates
% that *Simulink Design Verifier* has proven that we will always meet the 
% requirement as modeled in the *Field_Issue_Behavior* subsystem, and now
% the subsystem as a *green* outline indicating the proof objective was 
% proven valid.
%
% <<Step_07_PPreqProveResultsPass.png>>
%
% 
%% Summary
%
% During the *Property Proving* we have demonstrated that *Simulink Design
% Verifier* can be used to:
%
% * "Reactively" reproduce, debug and fix field issues
% * "Proactively" prove the implementation will meet requirements
%
% In both use cases, a test case and harness will be generated to help
% isolate and fix the issue the analysis uncovered.  Once the issue has 
% been resolved in the harness then we need to go back to the 
% *Property Proving* model to ask if *Simulink Design Verifier* can find 
% other test cases to falsify our proofs.  The key takeaway is that you 
% save the property models to rerun *Property Proving* on each new version 
% of the implementation.
%
% In the workshop we used *Property Proving* in a "reactive" manner to 
% show it can help with hard to reproduce field issues.  But going forward
% we would make *Property Proving* as part of "proactive" workflow where we
% would select several key or safety critical requirements to run on our
% implementation before our code generation/verification step.
%
% It is interesting to note that we diligently ran our requirements based
% simulation tests, passed the functional tests with 100% coverage and
% still there may be issues.  Formal methods test more of the design or
% operating space than simulation based test methods as shown below.
%
% <<Step_07_PP_DesignSpace.png>>
%
% The *Property Proving* step of the process is about having confidence in
% our design meeting requirements across a greater range of operating 
% conditions.  We have now answered all the questions as we have now 
% demonstrated a structured and formal testing framework for securing the 
% quality, robustness and safety of our cruise controller.
%
% <<Step_07_CruiseControl_Summary.png>>
%
% *_CONGRATULATIONS! YOU HAVE NOW COMPLETED ALL THE WORKSHOP EXERCISES!_*
%
% # When you are finished, close all models and files - or
% *<matlab:bdclose('all'); click here>*.
% # Go to *Step 8: Workflow Summary* - *<Step_08.html click here>*.
%
##### SOURCE END #####
--></body></html>